%%MO: changes other than compression
%% - changed notation for mean normalization in sect 5 to avoid confusion
%%   of two notions of mu
%% - moved tables out of the middle of paragraphs since that's visually
%%   easier for me to deal with in editing

\documentclass{kluwer}    % Specifies the document style.

\newdisplay{guess}{Conjecture}


\usepackage{graphicx}
\usepackage{qtree}

%\newcommand{\headlabel}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}}
\newcommand{\headlabel}[2]{
  \begin{tabular}{c} #1\\
    {\small #2} \end{tabular}}
\newcommand{\arclabel}[1]{\ensuremath{\stackrel{#1}{\to}}}

\newcommand{\precision}[1]{\ensuremath{\textrm{Prec}\left(#1\right)}}
\newcommand{\recall}[1]{\ensuremath{\textrm{Recall}\left(#1\right)}}


\begin{document}
\begin{article}
\begin{opening}         
\title{Expected Dependency Pair Match:
Predicting translation quality with expected syntactic structure} 
\author{Jeremy G. \surname{Kahn}\email{jgk@u.washington.edu}}  
\institute{University of Washington}
\author{Matthew \surname{Snover}\email{snover@cs.umd.edu}}
\institute{University of Maryland}
\author{Mari \surname{Ostendorf}\email{ostendor@u.washington.edu}}  
\institute{University of Washington}
\runningauthor{Kahn, Snover \& Ostendorf}
\runningtitle{Expected Dependency Pair Match}
%s\date{May 10, 2009}

\begin{abstract}
  Recent efforts to develop new machine translation
  evaluation methods have tried to 
  account for allowable wording differences either in terms of
  syntactic structure or synonyms/paraphrases. This paper primarily considers syntactic structure, combining scores from partial syntactic dependency
  matches with standard local n-gram matches using a statistical
  parser, and taking advantage of N-best parse probabilities in deriving
  scores for the hypothesized
  translation.  The new scoring metric, Expected Dependency
  Pair Match (EDPM), is shown to be superior to BLEU and TER in terms
  of correlation to human judgments and as a per-document and
  per-sentence predictor of HTER. Further, we combine the syntactic features of EDPM with the
  alternative wording features of TERp, showing a benefit to accounting for syntactic structure on top of
  semantic equivalency features.
\end{abstract}
\keywords{machine translation evaluation, syntax, dependency trees}

\end{opening}           

\section{Introduction}
\label{sec:intro}

A challenge in automatic machine translation (MT) evaluation is accounting
for allowable variability: two equally good
translations may be quite different in surface form. 
Currently, the most popular evaluation measures include a measure
based on $n$-gram precision known as BLEU \cite{papineni02bleu} and
the edit-distance measure Translation Edit Rate (TER)
\cite{snover06ter}.  These measures can only account for variability when 
given multiple translations, and studies show that they may
not accurately track translation quality both empirically
\cite{charniak03syntaxlmmt} and theoretically
\cite{callisonburch06bleuproblems}.

Alternative measures that 
incorporate synonym knowledge sources include: METEOR
\cite{banerjee05meteor}, which uses synonym tables and
morphological stemming to do progressively more forgiving matching;
%%MO: I am taking out the tuning stuff here since it isn't critical
%It can be tuned towards recall or precision, but is generally not
%tuned by users.  
TER Plus (TERp) \cite{snover09terp}, which is an extension of the
previously-mentioned TER that also incorporates synonym sets and stemming, along
with automatically-derived paraphrase tables.  
%TERp is explicitly
%intended to be tuned to a development set by users.
%
%Tuning has the advantage that the weight of different types of errors
%can be adjusted to match the needs of the task, though it makes it
%more difficult to compare results across tasks, particularly when
%there is little data for tuning.
Other metrics model syntactically-local (rather than string-local)
word-sequences include: tree-local
$n$-gram precision in various configurations of constituency and
dependency trees \cite{liu05syntaxformteval};\endnote{The dependency-based SParseval measure
\cite{roark06:sparseval}, designed as a parse-quality metric for
speech, is a similar approach, in that it is an F-measure over a
decomposition of reference and hypothesis trees.} and the \textbf{d} and
\textbf{d\_var} measures proposed by \inlinecite{owczarzak07evaluatingmt} that compare relational tuples derived from a
lexical functional grammar (LFG)
over reference and hypothesis translations.
%
These syntactically-oriented measures require a system for proposing
dependency structure over the reference and hypothesis
translations. \inlinecite{liu05syntaxformteval} use a
PCFG parser with deterministic head-finding, while 
\cite{owczarzak07evaluatingmt} extract the semantic dependency
relations from an LFG parser \cite{cahill04lfg}.
%
This work extends the dependency-scoring strategies of
\inlinecite{owczarzak07evaluatingmt}, which reported substantial
improvement in correlation with human judgment relative to BLEU and
TER, by using a 
publicly-available probabilistic context-free grammar (PCFG) parser and deterministic head-finding rules. In addition, we consider more types of constituents and different score combinations, as well as combination with of synonym-type scores.
 
MT measures are evaluated in a variety of ways. Some
\cite{banerjee05meteor,liu05syntaxformteval,owczarzak07evaluatingmt}
comparing the measure to human
judgments of fluency and adequacy.  In other work, e.g.\
\cite{snover06ter}, measures are compared to
human-targeted TER (HTER), a distance to a human-revised reference
that uses wording closer to the MT system choices (keeping the
original meaning) that is intended to measure the post-editing work
required after translation.  In this paper, we explore both kinds of
evaluation.

We describe our approach to including syntactic information in MT
evaluation by outlining a family of metrics in
section~\ref{sec:approach} and provide implementation details in
section~\ref{sec:paradigm}.  Section~\ref{sec:faexpts} examines the
correlation of several members of this family with human judgments of
fluency and adequacy, using the paradigm from
\cite{owczarzak07evaluatingmt} to provide comparisons to that work and
select a best case configuration, Expected Dependency Pair Match
(EDPM). The EDPM measure is then compared to BLEU and TER in terms of
correlation with HTER, exploring language/genre effects in
section~\ref{sec:hter1} and combination with TERp's synonym/paraphrase
features in section~\ref{sec:hter2}. Finally, findings and future work
are summarized in section~\ref{sec:conclusion}.

\section{Approach}
\label{sec:approach}


The specific family of dependency pair match (DPM) measures explored here
combines precision and recall scores of various
decompositions of a syntactic dependency tree. These measures are
extensions of the dependency-pair F measures found in
\inlinecite{owczarzak07labelleddepseval}\endnote{
  \inlinecite{owczarzak07evaluatingmt} extend their previous line of
  research \cite{owczarzak07labelleddepseval} by variably-weighting
%%MO: you actually do variable weighting using parse probabilities, so
%% would be good to mention what they use.  If you can say this in a few
%% words, then you won't add a line
  dependencies and by including synonym matching, two directions not
  pursued here.
}.  Rather than comparing
string sequences, as BLEU does with its $n$-gram precision, this
approach defers to a parser for an indication of the relevant word
tuples associated with meaning --- in these implementations, the head
on which that word depends.  Each sentence (both reference and
hypothesis) is converted to a labeled syntactic dependency tree and
then relations from each tree are extracted and compared.

We motivate the use of dependencies with actual translations:\\[4pt]
{\sl
Ref: Authorities have also closed southern Basra's airport and seaport.\\
S1: The authorities also closed the airport and seaport in the
southern port of Basra.\\
S2: Authorities closed the airport and the port of.\\[4pt]
}
A human judged the system 1 result (S1) as equivalent to the reference, but the system 2 (S2) result as
having errors.  BLEU gives both a similar score (0.199
vs.\ 0.203). TER scores S2 as better (errors
of 0.9 vs.  0.7, respectively), since a simple deletion requires fewer
edits that rephrasing.  By representing matches of dependencies, we
obtain a score for S1 from the new EDPM measure that is higher than
that for S2 (0.414 vs.\ 0.356).  The two phrases ``southern Basra's
airport and seaport'' and ``the airport and seaport in the southern
port of Basra'' have more in similarities in terms of dependencies
than word order.

The particular relations that are extracted from the dependency tree
are referred to here as \emph{decompositions}.
Figure~\ref{fig:decompexample}
illustrates the \emph{dependency-link-head} decomposition of a toy
dependency tree into a list of $\langle d, l, h \rangle$ tuples.  Some
members of the DPM family may apply more than one decomposition; other
good examples are the $dl$ decomposition, which generates a bag of
dependent words with outbound links, and the $lh$ decomposition, which
generates a bag of inbound link labels, with the head word for each
included. Figure~\ref{fig:decompexample2} shows the $dl$ and $lh$
decompositions for the same hypothesis tree.


\begin{figure}
  \centering
  \begin{tabular}{rcc}
    & \textbf{Reference} & \textbf{Hypothesis} \\
    \textbf{tree}
    & \includegraphics[scale=0.5]{dpm-example-ref} & 
    \includegraphics[scale=0.5]{dpm-example-hyp}\\
    \textbf{$dlh$ list} &
    \begin{tabular}{@{$\langle$}c@{,~}c@{,~}c@{$\rangle$}}
      the & \arclabel{\texttt{det}}  &  cat \\
      red & \arclabel{\texttt{mod}}  &  cat \\
      cat & \arclabel{\texttt{subj}} &  ate \\
      ate & \arclabel{\texttt{root}} &  $<$root$>$ \\
    \end{tabular} & 
    \begin{tabular}{@{$\langle$}c@{,~}c@{,~}c@{$\rangle$}}
      the &      \arclabel{\texttt{det}}  &  cat \\
      cat &      \arclabel{\texttt{subj}} &  stumbled \\
      stumbled & \arclabel{\texttt{root}} &  $<$root$>$ \\
    \end{tabular}\\
  \end{tabular}\\
%%MO: I don't think we need the P/R/F at this point in the narrative
%  Precision$_{dlh}$ is $\frac{1}{3}$ and Recall$_{dlh}$ is
%  $\frac{1}{4}$: thus $F[dlh] = \frac{2}{7}$
  \caption{Example hypothesis and reference dependency trees and the
    $dlh$ decomposition of each.}
  \label{fig:decompexample}
\end{figure}

\begin{figure}
  \centering
    \begin{tabular}{cc}
    $dl$ & $lh$ \\
    \hline
    \begin{tabular}{@{$\langle$}c@{,~}c@{$\rangle$}}
      the & \arclabel{\texttt{det}}  \\
      cat & \arclabel{\texttt{subj}} \\
      stumbled & \arclabel{\texttt{root}} \\
    \end{tabular} &
    \begin{tabular}{@{$\langle$}c@{,~}c@{$\rangle$}}
      \arclabel{\texttt{det} } &  cat \\
      \arclabel{\texttt{subj}} &  stumbled \\
      \arclabel{\texttt{root}} &  $<$root$>$ \\
    \end{tabular}\\
  \end{tabular}
  \caption{The $dl$ and $lh$ decompositions of the hypothesis tree in
    figure~\ref{fig:decompexample}. The items extracted here
    are individually less restrictive in their ability to match
    against reference tuples. 
%     $\precision{dl} = \frac{2}{3}$,
%     $\recall{dl} = \frac{2}{4}$, $\precision{lh} = \frac{2}{3}$,
%     $\recall{lh} = \frac{2}{4}$, so $F[dl,lh] = \frac{4}{7}$
  }
  \label{fig:decompexample2}
\end{figure}
Figure~\ref{fig:decompexample2} demonstrates the $dl$ and $lh$
decompositions for the hypothesis tree in figure~\ref{fig:decompexample}.

It is worth noting here that the $dlh$ and $lh$ decompositions (but
not the $dl$ decomposition) ``overweight'' the headwords, in that
there are $n$ elements in the resulting bag, but if a word has no
dependents it is found in the resulting bag exactly one time (in the
$dlh$ case) or not at all (in the $lh$ case).  Conversely,
syntactically ``key'' words, that are directly modified by many other
words in the tree, are included multiple times in the decomposition
(once for each inbound link).  We argue that this overcounting is a
virtue; the syntactic structure indicates which words are more
important to translate correctly.

We may not completely trust the parser's best parse.  The parser
itself, if we use a probabilistic parser, can provide an $n$-best
parse list for the translation reference and translation
hypothesis. We use the probability statistics of the list to compute
expected counts for each decomposition. Though this approach yields
partial counts, standard comparisons like precision and recall are
still valid, and using the expectations can help cope with both error
in the parser and ambiguity in the translations (reference and
hypothesis).

% Reviewer 2 suggests reducing/removing the use of \mu here to a basic
% explanation.

When multiple decomposition types are used together, we may combine
these subscores in a variety of ways.  We may compute precision and
recall subscores for each decomposition separately, or, since the
results of each decomposition are of different types entirely, we may
compute them as members of one large bag for an even simpler F score.
These two approaches are equivalent when only one decomposition type
is included.  For simplicity in presentation, we use the following
notation, where $dl$ and $lh$ represent the two kinds of
decompositions described above and $\mu_h$ represents a harmonic mean:
\begin{eqnarray}
  \label{eq:fprmeans}
  F[dl,lh] & = &
  \mu_h \left( \precision{dl \cup lh},
    \recall{dl \cup lh} \right) \\
  \mu_{PR}[dl,lh]  & = & \mu_h \left( \precision{dl},
    \recall{dl}, \precision{lh}, \recall{lh} \right)    
\end{eqnarray}
Dependency-based SParseval and the \textbf{d} approach from
\inlinecite{owczarzak07evaluatingmt} may each be understood as
$F[dlh]$, while the latter's \textbf{d\_var} method may be understood
as something close to $F[dl,lh]$.

Both of the combination methods above ($F[\cdot]$ and
$\mu_{PR}[\cdot]$) are ``naive'' in that they treat each component
score as equivalent to the next.  One further direction to explore
involves tuning (presumably on held-out data) the precision and recall
weights for each decomposition type for a later linear combination.

The possible family of metrics outlined above is quite large. In the
next section, we make explicit the range of these parameters that we
explore in this article.


\section{Experimental paradigm}
\label{sec:paradigm}
% In this section, we describe the details of the implementation and
% experimental variables explored in the experiments presented here.

\subsection{Parse tree implementation}
In principle, the family of DPM measures may be implemented with
any parser that generates a dependency graph (a single labelled arc
for each word, pointing to its head-word). Prior work
\cite{owczarzak07evaluatingmt} on related
measures has used an LFG parser \cite{cahill04lfg} or
an unlabelled dependency tree \cite{liu05syntaxformteval}. 

In this work, we use a state-of-the-art PCFG (the first stage of
\inlinecite{charniak-johnson:2005:ACL}) and context-free head-finding
rules \cite{magerman95headfinding} to generate a 50-best list of
dependency trees for each hypothesis and reference translation.  We
use the parser's default (English-language) Wall Street Journal
training parameters.  Head-finding uses the Charniak parser's rules,
with three modifications to make the semantic (rather than syntactic)
relations more dominant in the dependency tree: prepositional and
complementizer phrases choose nominal and verbal heads respectively
(rather than functional heads) and auxiliary verbs are modifiers of
main verbs (rather than the converse).

Having constructed the dependency tree, we label the arcs as $d
\stackrel{A/B}{\to} h$, where the arc label $A/B$ between dependent
$d$ and its head $h$ is composed of $A$ (the lowest constituent-label
headed by $h$ and dominating $d$) and $B$ (the highest constituent
label headed by $d$).
\begin{figure}
  \Tree
  [.\headlabel{S}{stumbled}
    [.\headlabel{NP}{cat}
      [.\headlabel{DT}{the} the ]
      [.\headlabel{NN}{cat} cat ]
    ]
    [.\headlabel{VP}{stumbled} 
      [.\headlabel{VBD}{stumbled} stumbled ]
    ]
  ]
  \\
  \begin{center}
    \includegraphics[scale=0.6]{dpm-example-depextract}
  \end{center}
  \caption{An example constituent tree (heads of each constituent are
    listed small below the label) and the labelled dependency tree
    derived from it using the strategy described in
    section~\ref{sec:paradigm}.}
  \label{fig:depextract}
\end{figure}
For example, in figure~\ref{fig:depextract}, the S node is the lowest
node headed by \emph{stumbled} that dominates \emph{cat}, and the NP
node is the highest constituent label headed by \emph{cat}, so the arc
between \emph{cat} and \emph{stumbled} is labelled \arclabel{S/NP}.
%
This strategy is very similar to one adopted in the reference
implementation of labelled-dependency \textsc{SParseval}, and may be
considered as an approximation of the rich semantics generated by LFG
parsers \cite{cahill04lfg} or another parser with a rich semantic
representation, but using a much shallower representation.

The $A/B$ labels are not as descriptive as the LFG semantics, but they
have a similar resolution, e.g.\ the $\stackrel{S/NP}{\to}$ arc label
usually represents a subject dependent of a sentential verb.

\subsection{Dimensions of metric family}
\label{sec:metricdimensions}
In the experiments presented later in this article, we explore the
space of metrics outlined above along multiple dimensions:
\begin{description}
\item[Decompositions.] We consider syntactic decompositions of the
  following types:
  \begin{description}
  \item[dlh] $\left\langle \textrm{Dependent}, \textrm{arc Label},
      \textrm{Head}\right\rangle$ Here, every tuple includes two words
    and the dependency-label between them.
  \item[dl] $\left\langle \textrm{Dependent}, \textrm{arc Label}
      \right\rangle$ The tuple is the word plus the label of the arc
      linking it to its head; this decomposition marks how the word
      fits into its syntactic context (what it modifies).
    \item[lh] $\left\langle \textrm{arc Label}, \textrm{Head}
      \right\rangle$ Each tuple here is the dependency relation plus
      the word being modified; this decomposition implicitly marks how
      key the word is to the sentence.
    \item[dh] $\left\langle \textrm{Dependent}, \textrm{Head}
      \right\rangle$ This decomposition drops the arc from the
      dependency-pair tuple, ignoring the syntactic-role information
      implied in the arc labels.
    \item[1g,2g] These decompositions are syntactically-null.  They
      simply include each 1-gram (or 2-gram) without any syntactic
      labeling at all.  They are thus simple measures of unigram
      (bigram) precision and recall.
  \end{description}
\item[Size of $n$-best list.] Although the parser can generate
  considerably more, we consider $n$-best lists of size 1 and of size
  50.
\item[Confidence of $n$-best hypotheses.] An expectation requires a
  probability distribution over the $n$-best list, and we consider
  three options: uniform, the parser probabilities, and a flattened
  version of the parser probabilities such that $\tilde{p}(x) =
  \frac{p(x)^\gamma}{\sum_ip(i)^\gamma}$ (where $\gamma$ is a free
  parameter) to account for the fact that the parser tends to be
  over-confident.  In all cases, the probabilities are normalized to
  sum to one over the the $n$-best list, where the maximum $n$ in this
  work is 50.  The uniform distribution ($\gamma = 0$) is intended to
  be equivalent to the \cite{owczarzak07evaluatingmt}
  \textbf{d\_50} and \textbf{d\_50\_pm} measures.\endnote{
    \inlinecite{owczarzak07evaluatingmt} report no use of parse
    confidence weights, and \textbf{d\_50} and \textbf{d\_50\_pm}
    return the best-matching pair of trees rather than an aggregate
    over the tree sets.}
\item[Score combination.] We consider three possible methods for
  combining scores across decompositions, as suggested in
  section~\ref{sec:approach}:
  \begin{description}
  \item[$F\lbrack\cdot\rbrack$] treats each decomposition as
    contributing $n$ more items to the bag of tokens extracted from
    the tree, and then computes a global $F$ score.
  \item[$\mu_{PR}\lbrack\cdot\rbrack$] constructs a precision and
    recall score from each decomposition, and then does an
    evenly-weighted harmonic mean of all the component precision and
    recall scores.
  \item Tuned weight combination, like the previous $\mu_{PR}$
    combination, calculates precision and
    recall separately for each decomposition but performs a linear
    weighted combination of the resulting precision and recall scores.
    For this tuning, we note that TERp \cite{snover09terp} provides a
    tuneable system over a small number of parameters (they include,
    among others, separate cost for insertions, deletions,
    substitutions and synonyms) and an optimization algorithm that
    assigns weights to each parameter. We use the various
    decompositions' precision and recall as new parameters to the TERp
    optimizer.
  \end{description}
\end{description}

\subsection{Reference evaluation measures over parallel translations}
\label{sec:metricsreference}
For some of the experiments presented here (sections~\ref{sec:hter1}
and~\ref{sec:hter2}), we have multiple translations of the same source
segment, each from a different translation engine and each with its
own HTER score.  For these translations, reporting correlations of
metric $m$ with absolute HTER scores can conflate $m$'s power in
identifying which of two candidate translations is better with $m$'s
(and HTER's) ability to distinguish which source segments are more
difficult to translate.

To avoid this conflation, on the HTER corpora we report correlations
on $\bar{\mu}m$ and $\bar{\mu}\textrm{HTER}$ (rather than measure
$m$ and HTER), where $\bar{\mu}$ represents mean-removal and $t_i$
represents the $i$th translation of segment $t$:
\begin{equation}
  \label{eq:meansub}
  \bar{\mu}m(t_i) = m(t_i) - \sum_{j=1}^I\frac{m(t_j)}{I}
\end{equation}
Mean-removal lowers the correlation $r$ values reported but ensures
that the reported correlations are among differences in the
translations rather than among differences in the underlying segments.
\endnote{Previous work \cite{kahn08metricsmatr} reported HTER
  correlations against pairwise differences among translation segments
  derived from the same source segment, unlike the mean-removal
  suggested in section~\ref{sec:metricsreference}. Those results are similar to
  those reported in section~\ref{sec:hter1}. However, we believe that pairwise differences
  introduce problems with the independence assumptions in the
  Pearson's $r$ tests, and we use mean-removal throughout this work
  instead.}

%% MO: Okay, but a little hard to figure out.
When reporting HTER correlations per-sentence, we use length-weighted
correlation, since unweighted correlations effectively put too much
emphasis on short sentences.  We do not weight for length for the
per-document correlations in section~\ref{sec:hter1}, because we
assume documents to be equally important (regardless of length).

\section{Correlation with human judgments of fluency \& adequacy}
\label{sec:faexpts}

To explore members of the DPM metric family, we explore the
correlations of various configurations against a corpus of human
judgments of fluency and adequacy.

\subsection{Corpus}
\label{sec:facorpus}
For these experiments, we use LDC Multiple Translation Chinese corpus
parts 2~\cite{LDC03MTC2} and 4~\cite{LDC06MTC4}, which are composed of
translations of written Chinese news stories.  These corpora
include multiple human judgments of fluency and adequacy for each
sentence, with each judgment using a different human judge and a
different reference translation.  Both fluency and adequacy
assessments are assigned on a five-point scale. For a rough\endnote{Our segment
  count reported in section~\ref{sec:facorpus} differs from
  \inlinecite{owczarzak07evaluatingmt}, who report 16,807 segments
  over the same corpus. We find baseline (BLEU and TER) per-segment correlations slightly
  different (BLEU$_4$ is higher here, while TER here is lower) than
  reported there as well, so the results
  presented here are not directly comparable with that paper, though
  we demonstrate similar gains over those baselines in essentially the
  same corpus.
} comparison with \inlinecite{owczarzak07evaluatingmt}, we treat
each judgment as a separate segment.
%
This treatment of this corpus yields 16,815 tuples of
$\langle$hypothesis, reference, fluency, adequacy$\rangle$.  In these
experiments, we extend this tuple with automatic scores derived from
$\langle$hypothesis, reference$\rangle$ and examine the per-segment
correlations\endnote{The independence of the translation segments in
  the Multiple Translation Corpus described in
  section~\ref{sec:facorpus} is questionable, since the same
  hypothesis translations are used in multiple items, but for the sake
  of methodological comparison with prior work, this strategy is
  preserved.}  between those automatic scores and the arithmetic mean
of the fluency and adequacy measures.

\subsection{Exploring decompositions}
We begin by comparing some of the simplest DPM measures to baseline
measures case-sensitive BLEU (4-grams, with add-one smoothing) and
TER.  For these first experiments, we consider only the 1-best parse
($n=1$).
\begin{table}
  \caption{Comparing $dl,lh$ to $dlh$ and baseline per-segment correlations with
    human fluency and adequacy judgments.}
  \label{tab:facorr:subgraphs}
  \begin{tabular*}{2.5in}{lr}
    \hline
    metric  &    \multicolumn{1}{c}{$|r|$} \\
    \hline
    $F[dl,lh]$ &   0.226 \\
    % # +1 smoothing
    BLEU$_4$ &   0.218 \\
    $F[dlh]$ &     0.185 \\
    TER &      0.173 \\
    \hline
  \end{tabular*}
\end{table}
In table~\ref{tab:facorr:subgraphs} we see that using the
partial-label $dl,lh$ decomposition has a better per-segment correlation with the
fluency/adequacy scores than TER or BLEU$_4$.  These results confirm,
with a PCFG, what \inlinecite{owczarzak07evaluatingmt} found with
an LFG parser: that partial-dependency matches are better correlated
with human judgments than full-dependency links.

In table~\ref{tab:facorr:combinations}, we compare among several other
members of the DPM family. 
 \begin{table}
  \begin{tabular*}{2.5in}{lr}
    \hline
    metric  &    \multicolumn{1}{c}{$r$} \\
    \hline
    $F[1g,2g,dl,lh]$      &  0.237 \\
    $\mu_{PR}[1g,2g,dl,lh]$ &  0.217 \\
    \rlcline{1-1} \rlcline{2-2}
    $F[1g,2g]$            & 0.227 \\
    $\mu_{PR}[1g,2g]$            & 0.215 \\
%    $F[1g,dl,dlh]$        & 0.227 \\
    \rlcline{1-1} \rlcline{2-2}
    $F[dl,lh]$            &  0.226 \\
    $\mu_{PR}[dl,lh]$      &  0.208 \\
    \hline
  \end{tabular*}
  \caption{Correlation with human fluency/adequacy judgments. Comparing combination methods $F[\cdot]$ and
    $\mu_{PR}[\cdot]$, and comparing the $1g,2g,dl,lh$ decomposition
    with the $dl,lh$ decomposition and other decompositions.}
  \label{tab:facorr:combinations}
\end{table}
Here, we find that combining the decompositions naively in $F[\cdot]$
(before computing precision and recall) has a better $r$ than the
harmonic mean of precision and recall subscores in $\mu_{PR}[\cdot]$.
In fact, $F[\cdot]$ measures always outperform the BLEU baseline (from
table~\ref{tab:facorr:subgraphs}), and $\mu_{PR}[\cdot]$ measures are
never better than BLEU.
We also see that we can combine the benefits of string-local $n$-grams
($F[1g,2g]$) with the benefits of dependency information ($F[dl,lh]$)
for a further improved correlation with human judgment, with the best
correlation in $F[1g,2g,dl,lh]$. Including progressively larger chunks
of the dependency graph with $F[1g,dl,dlh]$ (not shown in table, but inspired by
the BLEU$_k$ idea of progressively larger $n$-grams) does not seem to
be an improvement over $F[dl,lh]$.

% (too many chances for zeroes?)

\subsection{Exploring parse expectations}

To explore the effect of extracting parse feature-counts from a forest
instead of a single tree, we present in
table~\ref{tab:facorr:multiparse} several variants, with $n=1$ and
with $n=50$.
 \begin{table}
  \begin{tabular*}{2.5in}{lrr}
    \hline
    metric  & \multicolumn{1}{c}{$n$} &  \multicolumn{1}{c}{$r$} \\
    \hline
    $F[1g,2g,dl,lh]$      & 50 &  0.239 \\
    $F[1g,2g,dl,lh]$      &  1 &  0.237 \\
    \rlcline{1-2}\rlcline{3-3}
    $F[1g,dl,lh]$            &  50 & 0.237 \\
    $F[1g, dl,lh]$            &  1 & 0.234 \\
    \rlcline{1-2}\rlcline{3-3}
    $F[dl,lh]$            &  50 & 0.234 \\
    $F[dl,lh]$            &  1 & 0.226 \\
    \hline
  \end{tabular*}
  \caption{Correlation with human fluency/adequacy judgments. Comparing $n=1$ to $n=50$ for several variants of the DPM measure.}
  \label{tab:facorr:multiparse}
\end{table}
For the $n=50$ cases, we set $\gamma=0$ to assign uniform
probabilities to the parse forest, to compare as closely as possible
to \inlinecite{owczarzak07evaluatingmt}, which includes a
\textbf{d\_var\_50} measure with 50-best parses, with ranks but no
weights.  While not all of these differences are significant, the
trend is universally that the correlation $r$ improves as the number
of parses ($n$) is increased.  Tuning experiments find that increasing
$\gamma$ to 0.25 can increase the $r$ reported here for
$F[1g,2g,dl,lh]$ marginally (but insignificantly).

\subsection{Summary}

In this section, we have presented experiments exploring a number of
variants of the DPM metric against an average fluency/adequacy
judgment. The experiments suggest a best-case variant, where we set:
\begin{displaymath}
  \mbox{EDPM} = F[1g,2g,dl,lh], n=50, \gamma=0.25
\end{displaymath}
in which we choose a $1g,2g,dl,lh$ sub-graph decomposition based on
the improvements from better sub-graphs
(table~\ref{tab:facorr:subgraphs}), multiple parses ($n=50$) based on
table~\ref{tab:facorr:multiparse}, and $\gamma=0.25$.
%, hinted at by table~\ref{tab:facorr:parseweight}.  
We use these EDPM parameter-settings in the experiments exploring
document-level correlations with HTER (section~\ref{sec:hter1}).


\section{Correlating EDPM with HTER}
\label{sec:hter1}
In this section, we move from an exploration of the space of possible
DPM metrics to a single chosen DPM-derived metric (EDPM).  Here, we
compare EDPM to other metrics in correlation with document-level and
segment-level HTER performance.

\subsection{HTER corpus}

The GALE 2.5 translation corpus is made up of system translations into
English from three sites.
%
The three sites all use system combination to integrate results from
multiple systems, some of which are phrase-based and some which may
use syntax on either the source or target side. No system provided
system-generated parses.
%
The corpus being translated comes from Arabic and Chinese in four
genres: \texttt{bc} (broadcast conversation), \texttt{bn} (broadcast
news), \texttt{nw} (newswire), and \texttt{wb} (web text), with corpus
sizes shown in table~\ref{tab:galestats}.
\begin{table}
  \begin{tabular}{r|rr|rr|rr}
    \hline
     & \multicolumn{2}{c|}{Arabic} & \multicolumn{2}{c|}{Chinese}
     & \multicolumn{2}{c}{Total}\\
     & doc & sent & doc & sent & doc   & sent\\
     \hline
     \texttt{bc}    & 59  & 750 & 56 & 1061 & 115 & 1811\\
     \texttt{bn}    & 63  & 666 & 63 & 620  & 126 & 1286\\
     \texttt{nw}    & 68  & 494 & 70 & 440  & 138 & 934 \\
     \texttt{wb}    & 69  & 683 & 68 & 588  & 137 & 1271\\
     \hline
     Total & 259 & 2593& 257& 2709 & 516 & 5302\\
     \hline
  \end{tabular}
  \caption{Corpus statistics for the GALE 2.5 translation
    corpus.}
  \label{tab:galestats}
\end{table}
The corpus includes one English reference translation $r_i$
\cite{gale08phase2_5references} for each sentence $i$ and a system
translation $t_{i,z}$ for each of the three systems $z$. Additionally,
each of the system translations of each segment $i$ has a
corresponding human-targeted reference aligned at the sentence level,
so we have available the HTER score of each segment HTER$(t_{i,z})$ at
both the sentence and document level.

As discussed in section~\ref{sec:paradigm}, we report correlations
here between $\bar{\mu}m$ and $\bar{\mu}\textrm{HTER}$, rather
than $m$ and HTER directly, to abstract away from variations in the
underlying documents.

In table~\ref{tab:hterperdoc}, we show per-document Pearson's $r$
between $\bar{\mu}$EDPM and $\bar{\mu}$HTER, as well as two other
baselines: $\bar{\mu}$TER and $\bar{\mu}$BLEU$_4$.
\begin{subtable}
  \begin{table}
    \begin{tabular}{r|rrrr|rr|r}
      \hline
      $r$ vs. $\bar{\mu}$HTER & \multicolumn{1}{c}{\texttt{bc}}
      & \multicolumn{1}{c}{\texttt{bn}} &
      \multicolumn{1}{c}{\texttt{nw}} & \multicolumn{1}{c}{\texttt{wb}}
      & \multicolumn{1}{|c}{all Arabic} & \multicolumn{1}{c|}{all Chinese}
      & \multicolumn{1}{c}{all} \\
      \hline
      $\bar{\mu}$TER
      &  0.59 &  \textbf{0.35} &  \textbf{0.47}&  \textit{0.17}
      & \textbf{0.54} & \textbf{0.32} &  0.44\\
      $\bar{\mu}$BLEU
      & -0.42 & \textbf{-0.32} & \textbf{-0.46}&  \textbf{-0.27}
      & -0.42  &  \textbf{-0.33} & -0.37 \\
      $\bar{\mu}$EDPM
      & \textbf{-0.69} & \textbf{-0.39} & \textbf{-0.47} &
      \textbf{-0.27}
      & \textbf{-0.60} & \textbf{-0.39} & \textbf{-0.50} \\
      \hline
    \end{tabular}
    \caption{Per-document correlations of $\bar{\mu}$EDPM and others to
      $\bar{\mu}$HTER, by genre and by source language. Bold numbers are
      within 95\% significance of the best per column; italics indicate
      that the sign of the $r$ value has less than 95\% confidence.}
    \label{tab:hterperdoc}
  \end{table}
  \begin{table}
    \begin{tabular}{r|rrrr|rr|r}
      \hline
      $r$ vs. $\bar{\mu}$HTER & \multicolumn{1}{c}{\texttt{bc}}
      & \multicolumn{1}{c}{\texttt{bn}} &
      \multicolumn{1}{c}{\texttt{nw}} & \multicolumn{1}{c}{\texttt{wb}}
      & \multicolumn{1}{|c}{all Arabic} & \multicolumn{1}{c|}{all Chinese}
      & \multicolumn{1}{c}{all} \\
      \hline
      $\bar{\mu}$TER
      &  \textbf{0.44} &  \textbf{0.29} &  \textbf{0.33}&  0.25
      & \textbf{0.44} & 0.25  &  \textbf{0.36}\\
      $\bar{\mu}$BLEU
      & -0.31 & -0.24  & -0.29 &  -0.25
      & -0.31  &  -0.24  & -0.28 \\
      $\bar{\mu}$EDPM
      & \textbf{-0.46} & \textbf{-0.31} & \textbf{-0.34} &
      \textbf{-0.30}
      & \textbf{-0.44} & \textbf{-0.30} & \textbf{-0.37} \\
      \hline
    \end{tabular}
    \caption{Per-sentence, length-weighted correlations of
      $\bar{\mu}$EDPM and others to $\bar{\mu}$HTER, by genre and by
      source language.}
    \label{tab:hterpersent}
  \end{table}
\end{subtable}
We see that the EDPM has the highest correlation in each of the
subcorpora created by dividing by genre or by source language, as well
as the corpus as a whole.  Not every difference is above 95\%
confidence, but the trend is the same across all of the
subcorpora. 

%% MO: I still think this is a bit weird given that what you are
%% adding is structure. If time, you might want to look at some
%% example sentences
Table~\ref{tab:hterpersent} presents per-sentence correlations,
weighted by sentence length.  Though many of the $r$ values are
smaller magnitude in this correlation, EDPM again has the largest
correlation in each category.  TER, however, does quite well with
length-weighted sentence correlation, with $r$ values within 95\%
confidence of EDPM scores on nearly every breakdown.
%
In all cases in tables~\ref{tab:hterperdoc} and~\ref{tab:hterpersent},
EDPM is the most strongly correlated with HTER. In structured data
(\texttt{bn} and \texttt{nw}), these differences are not always
significant, but in the unstructured domains (\texttt{wb} and
\texttt{bc}), EDPM is always significantly better than at least one of
the comparison baselines.

\section{Weight-tuning to combine syntax and other knowledge sources}
\label{sec:hter2}

In the previous section, we observed that TER and EDPM each perform
better than BLEU when considering mean-removed length-weighted
correlations with HTER.  We observe that TER's basic operations
(insertions, deletions, shifts and substitutions) are fundamentally
different from those of EDPM, which is an $F$-style overlap measure
and is seeking to make explicit comparison based on an inferred
syntactic structure.  These two different approaches (word-level and
syntactic features) are complementary, and we seek in this section to
explore mechanisms for combining them.

As discussed in section~\ref{sec:intro}, the TER Plus (TERp) research
tools \cite{snover09terp} provide an optimizer for weighting multiple
simple subscores, including selecting weights for the TER basic
operations. TERp's feature list extends TER to additionally include
synonymy, stemming, and automatically-derived paraphrases.

%% MO: important to describe basic model before listing features.
The TERp optimizer performs a hill-climbing search, with randomized
restarts, to maximize the correlation of a linear combination of the
features with a set of human judgments.  Within the TERp framework the
features are the counts of the various edit types normalized for the
length of the reference so that the dot product of the features with a
vector of edit costs will give the TERp score for a given sentence.  In
these experiments, we follow~\inlinecite{snover09terp} by first
aligning the MT outputs with the reference, using TERp with a set of
default edit costs, to find the counts of the various edit types.
These alignments and edit costs are then held constant to optimize the
edit costs. In the case of comparing TERp without the paraphrasing
feature, a separate alignment was performed without that feature
enabled.  Additional features can be added to the TERp optimization
framework and optimized in the same manner as the edit costs.

In the experiments presented in this section, we extend this feature
list by including the syntactic overlap features, and we use the TERp
optimizer to tune the relative weight of each feature.

\subsection{Features}

We train a model using features from one or more of the following
sets:
\begin{description}
\item[E]: the fully syntactic features from the DPM family.
  Specifically, this feature set includes error-counts
  ($\frac{|\textrm{hyp}| - |\textrm{correct}|}{|\textrm{ref}|}$ and
  $\frac{|\textrm{ref}| - |\textrm{correct}|}{|\textrm{ref}|}$) 
  for the $dl$, $lh$, $dlh$, and $dh$ decompositions of the syntactic
  structure, using $n=50$ and $\gamma=0.25$.  The \textbf{E} set
  includes 8 features.
\item[N]: the non-syntactic features from the DPM family;
  specifically, error-counts for the $1g$ and $2g$ decompositions.
  The \textbf{N} set includes 4 features.
\item[T]: the features from basic TERp, excluding paraphrases, which include a separate cost
  for each of matches, insertions, deletions, substitutions, shifts, and
  synonym matches and stem matches. The \textbf{T} set includes 7 features.
\item[P]: features from the automatically-derived TERp paraphrase
  table. These are four features which are a function of the edit
  distance between the two phrases in each paraphrase and the
  probability of the paraphrase.
\item[B]: two ``length penalties'': brevity and prolixity. One is assigned the count of
  hypothesis words beyond the count of reference words, and one
  assigned the count of reference words beyond the count of hypothesis
  words.
  % MO: out of curiosity, why do you need 2 features?
  We assign two features here to let the system separately penalize
  ``too short'' translations and ``too long''.
\end{description}

\subsection{Corpus}

For these tuning experiments we use the same GALE 2.5 documents and
sentences from the previous section.  Since this approach requires
tuning, we assign documents randomly into two groups, such that each
group has the same document distribution across source-language and
genre.
%
These two corpus halves are used in two-fold cross-validation, testing
on one and training on the other.  We tune on (and report correlation
with) length-weighted per-sentence correlation with HTER, using
mean-removed scores as before.

% tuning target: weighted correlation with mean-removed segment HTER
\subsection{Effects of features on tuning}
% [we use weighted correlation to avoid emphasis on short segments
% (which hurts document-level and system-level utility) ]
In figure~\ref{fig:tuneresults}, we plot the Pearson's $r$ (with 95\%
confidence interval) for the results on the two test sets (each
trained on one and tested on the other).
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.45]{tuning.eps}
  \end{center}
  \caption{Pearson's $r$ values for various feature tunings, with 95\%
    confidence intervals. BLEU$_4$ (add-one smoothed), TER, and EDPM
    scores are provided on the left for comparison.}
  \label{fig:tuneresults}
\end{figure}
%
For each set of features, we plot the $r$ (and confidence) on both
halves of the corpus (for each half, tuned on the other half). In the
leftmost group of the figure, we include baselines: EDPM and BLEU and
TER (which are not tuned).  Next, the tuned E feature, which (by
itself) does not perform as well as TER or EDPM.
%
The remaining groups of three show that for each set of features, as E
(and EB) are included with others, correlation improves (though not
significantly).  These all move in the same direction. By the same
measurement, P features help beyond T -- as reported elsewhere -- but
N features do not seem to find a better correlation than TP features.
% MO suggestions:

Overall, we find that this approach for combining the underlying idea
of EDPM with the underlying ideas of TERp is promising, in that the E
features offer small (though non-significant) improvements to the
features already within TERp. However, this combination approach still
has more in common with the $\mu_{PR}$ combination method (explored in
section~\ref{sec:faexpts}) than with EDPM, which suggests a reason
that the EN features --- though tuned --- do not outperform EDPM.
%% do n-fold with >2 N -- better robustness to variation?

\section{Conclusion}
\label{sec:conclusion}
In this research, we explore dependency pair match (DPM) a family of
syntactically-oriented measures.  Through a corpus of human fluency
and adequacy judgments,
we settle on EDPM, a member of that family with promising predictive
power.  We find that EDPM is superior to BLEU and TER in terms of
correlation with human judgments and as a per-document and
per-sentence predictor of HTER (using mean-subtraction to account for
underlying document- and segment-level difficulty).
We experiment with including syntactic features in TERp-style tuning,
with linear weight combination, and find that including these features
improves correlation with mean-subtracted HTER, although these
improvements do not reach significance.

Several areas of future work are open to us. One obstacle to using
this method widely is the computational cost of parsing compared to
word-based measures
such as BLEU or TER.  Using these syntactic techniques may be possible
as a late-pass evaluation, to identify how candidate translation
systems are performing overall.  Alternatively, the various
tree-decompositions of the DPM family could be used as system
diagnostics, by looking at relative quality of these component scores
compared to those of an alternative candidate system.
%
Another possible approach is to store packed forests
\cite{huang08packedforests} rather than generating an $n$-best list
only to sum across it again in calculating the expectation.

Another research area relates to the quality of the parser.  In
this work, we demonstrated that the
\inlinecite{charniak-johnson:2005:ACL} PCFG parser could serve in
place of the LFG parser used in
\inlinecite{owczarzak07evaluatingmt}. PCFG parsers, however, are
fairly easily adaptable to another domain. One research
direction explores the intersection of genre (and genre-mismatch),
parser quality, and prediction of human performance.
% Another
% consideration is the speed of the parser: parse time currently takes
% much, much longer than BLEU, TER, or other word-based measures. 
We are interested in the trade-offs --- if any --- among parse
quality, genre adaptation, and predictive power as a translation metric.  

% \appendix
% [don't think we want an appendix]

\acknowledgements

{\small This material is based upon work supported by the National
  Science Foundation under Grant No. 0741585 and the Defense Advanced
  Research Projects Agency under Contract Nos. HR0011-06-C-0022 and
  HR0011-06-C-0023.

% Re: DARPA contract numbers
% -0022 is Matt's [Agile] grant.
% -0023 is Mari & Jeremy's [Nightingale] grant

  Any opinions, findings, and conclusions or recommendations expressed
  in this material are those of the author(s) and do not necessarily
  reflect the views of the National Science Foundation or the Defense
  Advanced Research Projects Agency.  }

% The endnotes section will be placed here.  But I don't think we have any

\theendnotes

\bibliographystyle{klunamed}
\bibliography{mtjournal}

\end{article}
\end{document}
