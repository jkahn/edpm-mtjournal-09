\documentclass{kluwer}    % Specifies the document style.

\newdisplay{guess}{Conjecture}

% TO DO: package provides \sout{} strikeout macro -- remove this
% before submission
\usepackage{ulem}

\usepackage{graphicx}
\usepackage{qtree}

%\newcommand{\headlabel}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}}
\newcommand{\headlabel}[2]{
  \begin{tabular}{c} #1\\
    {\small #2} \end{tabular}}
\newcommand{\arclabel}[1]{\ensuremath{\stackrel{#1}{\to}}}

\newcommand{\precision}[1]{\ensuremath{\textrm{Prec}\left(#1\right)}}
\newcommand{\recall}[1]{\ensuremath{\textrm{Recall}\left(#1\right)}}


\begin{document}
\begin{article}
\begin{opening}         
\title{Expected Dependency Pair Match:\\
Predicting {HTER} improvements with expected syntactic structure} 
\author{Jeremy G. \surname{Kahn}\email{jgk@u.washington.edu}}  
\institute{University of Washington}
\author{Matthew \surname{Snover}\email{snover@cs.umd.edu}}
\institute{University of Maryland}
\author{Mari \surname{Ostendorf}\email{ostendor@u.washington.edu}}  
\institute{University of Washington}
\runningauthor{Kahn, Snover \& Ostendorf}
\runningtitle{Expected Dependency Pair Match}
%s\date{May 10, 2009}

\begin{abstract}
Abstract to go here
\end{abstract}
\keywords{machine translation evaluation, syntax, dependency trees}

\end{opening}           

% override ulem 's default change of \emph{} -- remove before ship
\normalem
\section{Introduction}
\label{sec:intro}

Machine translation (MT) evaluation is a challenge for research
because the space of good translations is large, and two equally good
translations may appear to be quite different at first glance. 
%
The challenges of choosing among translations are compounded when this
evaluation is done automatically.
%
Human evaluation, however, is both time-consuming and difficult, so
research has turned increasingly towards automatic measures of
translation quality, usually by comparing the system translation to
one or more reference (human) translations.
%
Automatic measures of this kind (e.g.\ BLEU \cite{papineni02bleu}) not
only provide a well-defined evaluation standard but are also required
for training on error criteria, e.g.\ with minimum error rate training
\cite{och03mert}.

The most popular evaluation measures are currently BLEU (based on
$n$-gram precision) and the edit-distance measure Translation Edit
Rate (TER) \cite{snover06ter}.  Recent research has found that these
measures may not accurately track translation quality both empirically
\cite{charniak03syntaxlmmt} and theoretically
\cite{callisonburch06bleuproblems}.

These challenges have motivated a search for better measures that
incorporate additional language knowledge sources.  METEOR
\cite{banerjee05meteor}, for example, uses synonym tables and
morphological stemming to do progressively more-forgiving matching.
It can be tuned towards recall or precision, but is generally not
tuned by users.  TERp \cite{snover09terp} is an extension of the
previously-mentioned TER that also incorporates synonym sets, along
with automatically-derived paraphrase tables.  TERp is explicitly
intended to be tuned to a development set by users.

TO DO: DISCUSS TUNING'S VIRTUES, FLAWS

As an alternative to these synonym- and paraphrase-based approaches,
other metrics model syntactically-local (rather than string-local)
word-sequences. \cite{liu05syntaxformteval} compared tree-local
$n$-gram precision in various configurations of constituency and
dependency trees.  The dependent-based SParseval measure
\cite{roark06:sparseval}, originally designed as a parse-quality
metric for speech, is a similar approach, in that it is an F-measure
over a decomposition of reference and hypothesis trees.
\cite{owczarzak07labelleddepseval}'s \textbf{d} and \textbf{d\_var}
measures compare LFG-derived relational tuples from reference and
hypothesis translations and report substantial improvement in
correlation with human judgment relative to BLEU and TER.

These syntactically-oriented measures require a system for proposing
dependency structure over the reference and hypothesis
translations. Some \cite{liu05syntaxformteval,roark06:sparseval} use
PCFG parsers with deterministic head-finding, while others
\cite{owczarzak07labelleddepseval}extract the semantic dependency
relations from an LFG parser \cite{cahill04lfg}.
%
This work extends the dependency-scoring strategies of
\cite{roark06:sparseval,owczarzak07labelleddepseval} using a
widely-used and publically available PCFG parser and deterministic
head-finding rules.
 
We may evaluate automatic MT measures in a variety of ways. Some
\cite{banerjee05meteor,liu05syntaxformteval,owczarzak07labelleddepseval}
have evaluated their success by comparing the measure to human
judgments of fluency and adequacy.  In other work,
e.g. \cite{snover06ter}, measures are evaluated by comparison to
human-targeted TER (HTER), a distance to a human-revised reference
that uses wording closer to the MT system choices (keeping the
original meaning) that is intended to measure the post-editing work
required after translation.  In this paper, we explore both kinds of
evaluation.

TO DO: OVERVIEW OF THE REST OF THE PAPER



\section{Approach}
\label{sec:approach}

This work explores a family of dependency pair match (DPM) measures
that are composed of precision and recall combinations over various
decompositions of a syntactic dependency tree. These measures are
extensions of the dependency-pair F measures found in
\cite{roark06:sparseval,owczarzak07labelleddepseval}.  Rather than
comparing string sequences, as BLEU does with its $n$-gram precision,
this approach defers to a parser for an indication of the relevant
words that indicate more about its meaning --- in these
implementations, the head on which that word depends.  Each sentence
(both reference and hypothesis) is converted to a labeled syntactic
dependency tree and then relations from each tree are extracted and
compared.

A member of this family is defined by several parameters.  The first
of these parameters is the nature of the decomposition of the
dependency tree structure.  A \emph{decomposition list} is the list of
ways in which the tree is reduced to a bag of tree-local
tuples. Figure~\ref{fig:decompexample}
\begin{figure}
  \centering
  \begin{tabular}{rcc}
    & \textbf{Reference} & \textbf{Hypothesis} \\
    \textbf{tree}
    & \includegraphics[scale=0.5]{dpm-example-ref.eps} & 
    \includegraphics[scale=0.5]{dpm-example-hyp.eps}\\
    \textbf{$dlh$ list} &
    \begin{tabular}{@{$\langle$}c@{,~}c@{,~}c@{$\rangle$}}
      the & \arclabel{\texttt{det}}  &  cat \\
      red & \arclabel{\texttt{mod}}  &  cat \\
      cat & \arclabel{\texttt{subj}} &  ate \\
      ate & \arclabel{\texttt{root}} &  $<$root$>$ \\
    \end{tabular} & 
    \begin{tabular}{@{$\langle$}c@{,~}c@{,~}c@{$\rangle$}}
      the &      \arclabel{\texttt{det}}  &  cat \\
      cat &      \arclabel{\texttt{subj}} &  stumbled \\
      stumbled & \arclabel{\texttt{root}} &  $<$root$>$ \\
    \end{tabular}\\
  \end{tabular}\\
  Precision$_{dlh}$ is $\frac{1}{3}$ and Recall$_{dlh}$ is
  $\frac{1}{4}$: thus $F[dlh] = \frac{2}{7}$
  \caption{Example hypothesis and reference dependency trees and the $dlh$ decomposition of each.}
  \label{fig:decompexample}
\end{figure}
illustrates the \emph{dependency-link-head} decomposition of a toy
dependency tree into a list of $\langle d, l, h \rangle$ tuples.  Some
members of the DPM family may apply more than one decomposition; other
good examples are the $dl$ decomposition, which generates a bag of
dependent word with outbound links, and the $lh$ decomposition, which
generates a bag of inbound link labels, with the head word for each
included. 
\begin{figure}
  \centering
    \begin{tabular}{cc}
    $dl$ & $lh$ \\
    \hline
    \begin{tabular}{@{$\langle$}c@{,~}c@{$\rangle$}}
      the & \arclabel{\texttt{det}}  \\
      cat & \arclabel{\texttt{subj}} \\
      stumbled & \arclabel{\texttt{root}} \\
    \end{tabular} &
    \begin{tabular}{@{$\langle$}c@{,~}c@{$\rangle$}}
      \arclabel{\texttt{det} } &  cat \\
      \arclabel{\texttt{subj}} &  stumbled \\
      \arclabel{\texttt{root}} &  $<$root$>$ \\
    \end{tabular}\\
  \end{tabular}
  \caption{The $dl$ and $lh$ decompositions of the hypothesis tree in
    figure~\ref{fig:decompexample}. The items extracted here
    are individually less restrictive in their ability to match
    against reference tuples. 
%     $\precision{dl} = \frac{2}{3}$,
%     $\recall{dl} = \frac{2}{4}$, $\precision{lh} = \frac{2}{3}$,
%     $\recall{lh} = \frac{2}{4}$, so $F[dl,lh] = \frac{4}{7}$
  }
  \label{fig:decompexample2}
\end{figure}
Figure~\ref{fig:decompexample2} demonstrates the $dl,lh$
decomposition for the hypothesis tree in figure~\ref{fig:decompexample}.

It is worth noting here that the $dlh$ and $lh$ decompositions (but
not the $dl$ decomposition) ``overweight'' the headwords, in that
there are $n$ elements in the resulting bag, but if a word has no
dependents it is found in the resulting bag exactly one time (in the
$dlh$ case) or not at all (in the $lh$ case).  Conversely,
syntactically ``key'' words, that are directly modified by many other
words in the tree, are included multiple times in the decomposition
(once for each inbound link).  We argue that this overcounting is a
virtue; the syntactic structure indicates which words are more
important to translate correctly.

We may not completely trust the parser's best parse.  The parser
itself, if we use a probabilistic parser, can provide an $n$-best
parse list forest for the translation reference and translation
hypothesis. We use the probability statistics of the forest to compute
expected counts for each decomposition. Though this approach yields
partial counts, standard comparisons like precision and recall are
still valid, and using the expectations can help cope with both error
in the parser and ambiguity in the translations (reference and
hypothesis).

When multiple decomposition types used together, we may combine these
subscores in a variety of ways.  We may compute precision and recall
subscores for each decomposition separately, or, since the results of
each decomposition are of different types entirely, we may compute
them as members of one large bag for an even simpler F score.  These
two approaches are equivalent when only one decomposition type is
included.  For simplicity in presentation, we use the following
notation, where $dl$ and $lh$ represent the two kinds of
decompositions described above and $\mu_h$ represents a harmonic mean:
\begin{eqnarray}
  \label{eq:fprmeans}
  F[dl,lh] & = &
  \mu_h \left( \precision{dl \cup lh},
    \recall{dl \cup lh} \right) \\
  \mu_{PR}[dl,lh]  & = & \mu_h \left( \precision{dl},
    \recall{dl}, \precision{lh}, \recall{lh} \right)    
\end{eqnarray}
Dependency-based SParseval and Owczarzak \textit{et al.}'s
\cite*{owczarzak07labelleddepseval} \textbf{d} approach, then, may
each be understood as $F[dlh]$, while the latter's \textbf{d\_var}
method may be understood as something close to $F[dl,lh]$.

Both of the combination methods above ($F[\cdot]$ and
$\mu_{PR}[\cdot]$) are ``naive'' combinations, in that they treat each
decomposition as equivalent to the next.  One further direction to
explore involves tuning (presumably on held-out data) the precision
and recall weights for each decomposition type for a later linear
combination.

The possible family of metrics outlined above is quite large. In the
next section, we make explicit the range of these parameters that we
explore in this article.


\section{Experimental paradigm}
\label{sec:paradigm}
In this section, we outline the details of the implementation and
experimental variables explored in the experiments presented here.

\subsection{Parse tree implementation}
In principle, the family of DPM measures may be implemented with
any parser that generates a dependency graph (a single labelled arc
for each word, pointing to its head-word). Prior work
\cite{owczarzak07labelleddepseval} on related
measures has used an LFG parser \cite{cahill04lfg} or
an unlabelled dependency tree \cite{liu05syntaxformteval}. 

%
In this work, we use a state-of-the-art PCFG (the first stage of
\cite{charniak-johnson:2005:ACL}) and context-free head-finding rules
\cite{magerman95headfinding} to generate a 50-best list of dependency
trees for each hypothesis and reference translation.  We use the
parser's default (English-language) Wall Street Journal training
parameters.  Head-finding uses the Charniak parser's rules, with three
modifications: prepositional and complementizer phrases choose nominal
and verbal heads respectively (rather than functional heads) and
auxiliary verbs are modifiers of main verbs (rather than the
converse).

Having constructed the dependency tree, we label the arcs as $d
\stackrel{A/B}{\to} h$, where the arc label $A/B$ between dependent
$d$ and its head $h$ is composed of $A$ (the lowest constituent-label
headed by $h$ and dominating $d$) and $B$ (the highest constituent
label headed by $d$). 
\begin{figure}
  % \centering
  \Tree
  [.\headlabel{S}{stumbled}
    [.\headlabel{NP}{cat}
      [.\headlabel{DT}{the} the ]
      [.\headlabel{NN}{cat} cat ]
    ]
    [.\headlabel{VP}{stumbled} 
      [.\headlabel{VBD}{stumbled} stumbled ]
    ]
  ]
  \\
  \begin{center}
    \includegraphics[scale=0.6]{dpm-example-depextract.eps}
  \end{center}
  \caption{An example constituent tree (heads of each constituent are
    listed small below the label) and the labelled dependency tree
    derived from it using the strategy described in
    section~\ref{sec:paradigm}.}
  \label{fig:depextract}
\end{figure}
For example, in figure~\ref{fig:depextract}, the S node is the
lowest node headed by \emph{stumbled} that dominates \emph{cat}, and
the NP node is the highest constituent label headed by \emph{cat}, so
the arc between \emph{cat} and \emph{stumbled} is labelled \arclabel{S/NP}.
%
This strategy is very similar to one adopted in the reference
implementation of labelled-dependency \textsc{SParseval}, and may be
considered as an approximation of the rich semantics generated by
\cite{cahill04lfg} or another heavily knowledge-engineered
parser, but with much less knowledge-engineering required.

The $A/B$ labels are not as descriptive as the LFG semantics, but they
have a similar resolution, e.g.\ the $\stackrel{S/NP}{\to}$ arc label
usually represents a subject dependent of a sentential verb.

\subsection{Dimensions of metric family}
In the experiments presented later in this article, we explore the
space of metrics outlined above along multiple dimensions:
\begin{description}
\item[Decompositions]: we consider syntactic decompositions of the
  following types.
  \begin{description}
  \item[dlh] $\left\langle \textrm{Dependent}, \textrm{arc Label},
      \textrm{Head}\right\rangle$ here, every tuple includes two words
    and the dependency-label between them.
  \item[dl] $\left\langle \textrm{Dependent}, \textrm{arc Label}
      \right\rangle$ the tuple is the word plus the label of the arc
      linking it to its head; this decomposition marks how the word
      fits into its syntactic context (what it modifies).
    \item[lh] $\left\langle \textrm{arc Label}, \textrm{Head}
      \right\rangle$ each tuple here is the dependency relation plus
      the word being modified; this decomposition implicitly marks how
      key the word is to the sentence.
    \item[dh] $\left\langle \textrm{Dependent}, \textrm{Head}
      \right\rangle$ here, drops the arc-label. This decomposition
      ignores the syntactic-role information implied in the arc labels.
    \item[1g,2g] these decompositions are syntactically-null.  They
      simply include each 1-gram (or 2-gram) without any syntactic
      labeling at all.  They are thus simple measures of unigram
      (bigram) precision and recall.
  \end{description}
\item[Forest size] Although the parser can generate considerably more,
  we consider forests of size 1 and of size 50.
\item[Forest confidence] An expectation requires a probability
  distribution over the $n$-best list, and we consider three options:
  uniform, the parser probabilities, and a flattened version of the
  parser probabilities such that $\tilde{p}(x) =
  \frac{p(x)^\gamma}{\sum_ip(i)^\gamma}$ (where $\gamma$ is a free
  parameter) to account for the fact that the parser tends to be
  over-confident.  In all cases, the probabilities are normalized to
  sum to one over the the $n$-best list, where the maximum $n$ in this
  work is 50.  The uniform distribution ($\gamma = 0$) is intended to
  be equivalent to the \cite{owczarzak07labelleddepseval}
  \textbf{d\_50} and \textbf{d\_50\_var} measures.\footnote{Since
    \cite{owczarzak07labelleddepseval} report no use of parse weights,
    \textbf{d\_50} and \textbf{d\_50\_var} may be using a sum of
    counts over the 50-best list rather than expected-counts over a
    uniform distribution. These two approaches are equivalent --- so
    long as the $n$-best list is always the same length for hypothesis
    and reference.  In our implementation, the $n$-best list does not
    always reach 50 candidate parses on short sentences, so the
    expectation matches our intent better than a sum of counts over
    the $n$-best.}
\item[score combination]: We consider three possible methods for
  combining scores across decompositions, as suggested in
  section~\ref{sec:approach}:
  \begin{description}
  \item[$F\lbrack\cdot\rbrack$] treats each decomposition as
    contributing $n$ more items to the bag of tokens extracted from
    the tree, ad then computes a global $F$ score.
  \item[$\mu_{PR}\lbrack\cdot\rbrack$] constructs a precision and
    recall score from each decomposition, and then does an
    evenly-weighted harmonic mean of all the component precision and
    recall scores.
  \item Tuned, like the previous combination, calculates precision and
    recall separately for each decomposition, but performs a linear
    weighted combination of the resulting precision and recall scores.
    For this tuning, we note that TERp \cite{snover09terp} provides a
    tuneable system over a small number of parameters (they include,
    among others, separate cost for insertions, deletions,
    substitutions and synonyms) and an optimization algorithm that
    assigns weights to each parameter. We use the various
    decompositions' precision and recall as new parameters to the TERp
    optimizer.
  \end{description}
\end{description}

\subsection{Reference evaluation measures over parallel translations}

For some of the experiments presented here (sections~\ref{sec:hter1}
and~\ref{sec:hter2}), we have multiple translations of the same source
segment, each from a different translation engine and each with its
own HTER score.  For these translations, reporting correlations of
metric $m$ with absolute HTER scores can conflate $m$'s power in
identifying which of two candidate translations is better with $m$'s
(and HTER)'s ability to distinguish which source segments are
more difficult to translate. 

To avoid this conflation, on the HTER corpora we report correlations of
the mean-removed $m$ and HTER scores. Mean-removal  lowers the
correlation $r$ values reported but
ensures that the reported correlations are among differences in the
translations rather than among differences in the underlying segments.

Section~\ref{sec:hter2} reports HTER correlations per-sentence. There,
we use length-weighted correlation, since unweighted correlations
effectively puts too much emphasis on short sentences.  We do not
weight for length in section~\ref{sec:hter1}, which reports document
correlations, because documents are assumed to be equally important
(regardless of length).

TO DO: MENTION DELTAS IN A FOOTNOTE, AND CITE MetricsMATR SYSTEM PAPER


\section{Correlation with human judgments of fluency \& adequacy}
\label{sec:faexpts}

TO DO: INTRODUCE THIS SECTION

\subsection{Corpus}

For these experiments, we use LDC Multiple Translation Chinese corpus
parts 2~\cite{LDC03MTC2} and 4~\cite{LDC06MTC4}.  These corpora
include multiple human judgments of fluency and adequacy for each
sentence, with each judgment using a different human judge and a
different reference translation.  For a rough\footnote{Our segment
  count differs from \cite{owczarzak07labelleddepseval}, who report
  16,800 segments over the same corpus. We find baseline correlations
  (BLEU$_4$ and TER) lower than those reported there as well, so the
  results presented here are not directly comparable with that paper,
  though we demonstrate similar gains over those baselines in
  essentially the same corpus. % (section~\ref{sec:facorr:newdeps}).}
}
comparison with \cite{owczarzak07labelleddepseval}, we treat each
judgment as a separate segment.
%
This treatment of this corpus yields 16,815 tuples of
$\langle$hypothesis, reference, fluency, adequacy$\rangle$.  In these
experiments, we extend this tuple with automatic scores derived from
$\langle$hypothesis, reference$\rangle$ and examine the
correlations\footnote{The independence of each of these segments is
  questionable, since the same hypothesis translations are used in
  multiple items, but for the sake of methodological comparison with
  prior work, this strategy is preserved.}  between those automatic
scores and the arithmetic mean of the fluency and adequacy measures.


  \begin{table}
    % \centering
    \caption{$dl,lh$ decompositions are
      more correlated with the fluency-adequacy average than BLEU$_4$
      or $dlh$ decompositions.}
    \begin{tabular}{lr}
      metric  &    $r$ \\
      \hline
      $F[dl,lh]$ &   0.226 \\
       % # +1 smoothing
      BLEU$_4$ &   0.218 \\
      $F[dlh]$ &     0.185 \\
      TER &      -0.173 \\
    \end{tabular}
    \label{tab:facorr:subgraphs}
\end{table}
      
Table~\ref{tab:facorr:subgraphs} indicates that: using
partial-labeling (dl,lh) much better than full link. Also, using
Charniak parser as labeler seems to work, LFG not necessary
Here, $n=1$ and only the $F[\cdot]$ score combination is shown.

\begin{itemize}

\item EXPT 
  combining precision, recall naively vs. combining items

\begin{verbatim}
  F(1g,2g,dl,lh)        0.237
  prmeans(1g,2g,dl,lh)  0.217

  F(dl,lh)              0.226
  prmeans(dl,lh)        0.208
\end{verbatim}

  point: combining individual items naively (before computing
  precision and recall) seems better than naively combining precisions
  and recalls (too many chances for zeroes?)

\item EXPT
  \label{tab:facorr:multiparse}
  expanding n, gamma=0

\begin{verbatim}
  F[1g,2g,dl,lh],n=50   0.239
    ...          n=1    0.237
  F[dl,lh],n=50         0.234
    ...    n=1          0.226
\end{verbatim}
  point: larger n $\to$ better r
  (holds for other comparisons too, but these are good examples)

\item  EXPT
\label{tab:facorr:parseweight}
  setting gamma

  Tuning experiments finds that gamma of 0.25 is slightly better
  (especially when using F[dl,lh]) than gammas of other values in
  range 0-1 (these values probably not significant)

\subsection{Summary}

In this section, we have presented experiments exploring a number of
variants of the DPM metric against an average fluency/adequacy judgment. The experiments suggest a
best-case variant, where we set:
\begin{displaymath}
  EDPM = F[1g,2g,dl,lh], n=50, \gamma=0.25
\end{displaymath}
in which we choose a $1g,2g,dl,lh$ sub-graph decomposition based on
the improvements from better sub-graphs
(table~\ref{tab:facorr:subgraphs}), multiple parses ($n=50$) based on
table~\ref{tab:facorr:multiparse}, and $\gamma=0.25$, hinted at by
table~\ref{tab:facorr:parseweight}.  We use these EDPM
parameter-settings in the experiments exploring document-level correlations with HTER
(section~\ref{sec:hter1}).


\end{itemize}

\section{Per-document correlations with HTER}
\label{sec:hter1}
TO DO: INTRODUCE THIS SECTION

The GALE 2.5 translation corpus is made up of system translations into
English from three sites.
% MO added:
The three sites all use system combination to integrate results from
multiple systems, some of which are phrase-based and some which may
use syntax on either the source or target side. No system provided
system-generated parses.
%
The corpus being translated comes from
Arabic and Chinese in four genres: \texttt{bc} (broadcast
conversation), \texttt{bn} (broadcast news), \texttt{nw} (newswire),
and \texttt{wb} (web text), with corpus sizes shown in
table~\ref{tab:galestats}.
\begin{table}
%  \centering
  \begin{tabular}{r|rr|rr|rr}
     & \multicolumn{2}{c|}{Arabic} & \multicolumn{2}{c|}{Chinese}
     & \multicolumn{2}{c}{Total}\\
     & doc & sent & doc & sent & doc   & sent\\
     \hline
     \texttt{bc}    & 59  & 750 & 56 & 1061 & 115 & 1811\\
     \texttt{bn}    & 63  & 666 & 63 & 620  & 126 & 1286\\
     \texttt{nw}    & 68  & 494 & 70 & 440  & 138 & 934 \\
     \texttt{wb}    & 69  & 683 & 68 & 588  & 137 & 1271\\
     \hline
     Total & 259 & 2593& 257& 2709 & 516 & 5302\\
  \end{tabular}
  \caption{Corpus statistics for the GALE 2.5 translation
    corpus.}
  \label{tab:galestats}
\end{table}
The corpus includes one English reference translation $r_i$
\cite{gale08phase2_5references} for each sentence $i$ and a system
translation $t_{i,z}$ for each of the three systems
$z$. Additionally, each of the system translations of each segment $i$
has a corresponding human-targeted reference aligned at the sentence
level, so we have available the HTER score of each segment
$s_{\textsc{hter}}(t_{i,z})$ at both the sentence and document level.


  prediction:
    Mean-subtracted HTER per-document

  base measure EDPM: F[1g,2g,dl,lh],n=50,gamma=0.25

  [EXPT]
\begin{verbatim}
            Ar     Zh    All
   TER     0.51   0.32  0.44
   BLEU_4 -0.42  -0.33 -0.37
   EDPM   -0.60* -0.39 -0.50*
\end{verbatim}
  point: base EDPM does better on all docs, and on Arabic. difference
  is not quite significant at the p=0.05 on the Chinese documents

  Note 1: also tried pairwise deltas per-document, with similar
  results, as presented in the MetricsMATR competition submission.

  Note 2: when using per-segment mean-subtracted, r values are smaller
  and *Arabic* was the language where EDPM did not perform best.

\begin{verbatim}
               Ar     Zh     All
      TER     0.38*  0.04   0.19
      BLEU_4 -0.10  -0.16  -0.14
      EDPM   -0.31  -0.19* -0.24*
\end{verbatim}


  include genre tables here as well -- cut out again if no story.

\section{Weight-tuning to combine syntax and other knowledge sources}
\label{sec:hter2}

TERp optimizer [Matt, a brief description here]

TERp comes with a variety of features (described earlier)

include optionally new features

Corpus: GALE 2.5 data, again documents randomly split into two groups,
evenly split across language and genre

tuning target: weighted correlation with mean-removed segment HTER

[we use weighted correlation to avoid emphasis on short segments
(which hurts document-level and system-level utility) ]

Select features from set:

\begin{description}
\item[E]: features from EDPM, specifically, P,R for inbound, outbound,
  full-links, and unlabeled links  (8 features)
\item[T]: features from basic TERp
  (inserts, deletes, substitutions, shifts)  (7 features)
\item[P]: features from TERp paraphraser
  (4 features)
\item[N]: precision and recall for 1-grams, 2-grams
\item[B]: brevity/prolixity (2 features: one for longer-than-ref, one for
  shorter-than-ref)
\end{description}

tune weights on one set, test on the other (and vice versa).
reporting average r between the two.

\begin{verbatim}
   == Tuned For Weighted Mean Removed Pearson
     - Examining WGT_MEAN_RM_SEG_PEAR ==
            Average
      Cond   Test  
     ---------------
      ETPB   0.4096
      ETP    0.4038
      TPB    0.4010
      TP     0.3946
      TPNB   0.3940
      TPN    0.3891
      ETB    0.3871
      ET     0.3827
      TB     0.3815
      ETNB   0.3814
      ETN    0.3804
      T      0.3758
      TNB    0.3708
      TN     0.3680
      E      0.3287
      EB     0.3267
      ENB    0.3149
      NB     0.3058
      EN     0.2973
      N      0.2636
      B      0.1638
\end{verbatim}
(maybe don't report all these numbers!)
  
point: E+T $>$ T, E+TB $>$ TB, E+TP $>$ TP; information is available in
syntax that is not captured in the other measures.

point: syntax is *not* just an expensive way to get at n-grams;
N-grams are not the same -- TP $>$ TP+N, but TP $<$ TP+E

\section{Conclusion}
\label{sec:conclusion}
expected dependency pair matching gets at something new

troubling area: speed of analysis -- current implementation requires
running a complete n-best parser

possible use: as a late-pass evaluation, to identify how systems
perform overall

future work:  explore ways to get at syntactic information without
the expense of a full parser:
\begin{itemize}
\item this work moves from an LFG parser (Owczarzak) to the
  substantially simpler and more robust Charniak system - keep going
  and look at simpler partial-parsing approaches (forests?)
\item conversely: how important is it that the parses be of
  high-quality?
\end{itemize}





\appendix

[don't think we want an appendix]


\acknowledgements

[grant numbers for the DARPA/GALE grants here]

Matthew Snover is supported under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022.

% The endnotes section will be placed here.  But I don't think we have any

% \theendnotes

\bibliographystyle{klunamed}
\bibliography{edpm-paper}

\end{article}
\end{document}
