% -*- LaTeX -*-
\documentclass[letterpaper,12pt]{article}

\newenvironment{response}
{\begin{quotation} \it}
  {\end{quotation}}

\begin{document}

Note from the authors: all references to sections and tables in our responses are with respect to numbering in the revised version of the paper.

\section{Comments for the Author: Reviewer \#1}
\label{sec:reviewer1}

This submission on Dependency Pair Match MT metrics is technically and
structurally sound, thorough, and well-written.  Some specific
comments/questions below, in order of appearance in the paper.
\begin{response}
  Thank you for this very useful feedback. Our responses are below.
\end{response}
As per the submission guidelines, submissions must be limited to 12
pages of content, with an additional page for references.  The current
version has 16.5 pages plus one page of references, so significant
shortening will be necessary.  I find the overall balance of sections
adequate, so I would recommend shortening across the board rather than
in one particular section.
\begin{response}
  We have done revisions to compress and simplify the writing
  throughout the paper to address the length concerns.
\end{response}
While likely known by most of the community, I would recommend
expanding all acronyms the first time (LFG, PCFG, ...).
\begin{response}
  Done.
\end{response}

In section 2, ``bag of dependent word'' should be ``bag of dependent
word*s*''.
\begin{response}
  Done.
\end{response}
Can you expand on the reasoning behind the modifications to the
Charniak parser rules described in section 3.1?
\begin{response}
  We added discussion in the second paragraph of section 3.1 and refer back to the new example in
  section two, which illustrates this point. 
\end{response}
In section 4.1, while the corpus is referenced and the information can
be looked up, it would still be helpful to include directly in the
paper a brief description of the adequacy and fluency measures.  In
particular, what kind (granularity) of scale was used for the
judgments needs to be known in order to be able to assess the reported
correlations.  A brief description of the data genre(s) would be
useful to have at hand as well.  Also, I assume you correlate at the
*segment* level, but this is not explicitly stated.
\begin{response}
  Section 4 has now included information on granularity (5-point scale),
genre (Chinese news stories), and a note that these are per-segment correlations.
\end{response}
The correlations reported in section 4 (tables I-III) are overall
quite low (also compared to EDPM's r correlations for Chinese source
data in NIST's MetricsMATR challenge).  Could you comment on this?
\begin{response}
 The correlations are similar to those reported in Owczarzak et al., and they
are low in part because of the fact that they are computed at the segment level.
We added a parenthetical in section 5, paragraph 4, indicating that sentence-level correlations are lower than document-level correlations because of the greater variability.  Of course, there are also differences simply associated with the fact that these are different data sets.
\end{response}
For ease of lookup, please include in the table headings for tables
I-III that the correlations are with human judgments of adequacy and
fluency
\begin{response}
  Done.
\end{response}

In section 5.1, ``EDPM's strengths, relative to the other measures, are
particularly clear in the unstructured domains (wb and bc).''  This
statement does not seem to hold for EDPM vs. BLEU for the wb genre in
table Va; the correlation reported is the same for the two metrics?
\begin{response}
  The sentence at the end of the second-to-last paragraph of section 5
  is revised to indicate that ``In structured data (\texttt{bn} and
  \texttt{nw}), these differences are not always significant, but in
  the unstructured domains (\texttt{wb} and \texttt{bc}), EDPM is
  always significantly better than at least one of the comparison
  baselines.''
\end{response}

\section{Comments for the Author: Reviewer \#2}
\label{sec:reviewer2}

This article presents a method for evaluating MT system hypotheses
that is based on complete and partial dependency triple matches
between hypothesis and reference translation.

The paper is well motivated, clearly written, and presents interesting
results. On my opinion, it is suitable for publication with minor
revisions. 
\begin{response}
  Thank you for your detailed critique and many useful
  suggestions. Our responses are below:
\end{response}
My main points of criticism are:

\begin{itemize}
\item It is considerably too long. Papers are supposed to be 12 pages
  + references; this paper has more than 16 pages. I recommend (a)
  shortening the introduction; (b) shortening the discussion of the
  ``model space'' in Sections 2/3 somewhat; (c) removing the third
  experiment (Section 6), whose contribution is unclear to me,
  altogether. (see below for details)
  \begin{response}
    Our revisions incorporate many of these suggestions. Rather than
    eliminate section 6, we have chosen to compress it, but we have
    adopted the rest of the suggestions above.  Details about the 
    changes to section 6 are in the response to the last comment.  We think that these 
    changes make the contribution
    of section 6 more clear.
  \end{response}

\item On the other hand, the paper does not motivate at all what the
  intuition behind the use of dependency triple match is. Why should
  we assume in the first place that dependency triples predict human
  judgments better than (e.g.) n-grams?
  \begin{response}
    We include an example in section 2 to address this point. 
  \end{response}

\item Finally, the paper does not give a single example. In a paper
  about a novel evaluation metric, I would like to get a feeling for
  what it is that the new metric does better than existing
  metrics. (This can be addressed together with the previous point.)
  \begin{response}
    Done in the section 2 example.
  \end{response}
\end{itemize}

Detail comments:

\begin{itemize}
\item The discussion on page 2 reads nicely, but is probably too
  extensive -- in particular for a paper aimed at a special issue
  where there will presumably be an overview article that discusses
  relevant previous work.
  \begin{response}
    We have compressed this discussion considerably.
  \end{response}

\item  p.2, publically $\to$ publicly?
  \begin{response}
    Done.
  \end{response}

\item  p.3, ``In section 2 etc.'' I'm not sure that a 12-page paper
  requires a table of contents.
  \begin{response}
    This paragraph has also been compressed.
  \end{response}

\item  p.3, reference to Owczarzak et al.'s work. In particular since
  their work has also appeared in the MT journal (please update the
  reference, btw!), it would be good to expand on the similarities and
  differences of your work and theirs.
  \begin{response}
    Both references are cited in the introduction.  In the comparative
    discussions, the earlier reference is cited since we were not aware of
    the MT journal paper when doing the work and experiments were designed
    with the first paper in mind.  We feel that our paper did have 
    discussion of the similarities and differences, but with the compression
    of other material these are better highlighted in the revisions. In addition, the last
    paragraph of section 1 now states that section 4 is in part aimed at making
    the comparison.
  \end{response}

\item p.4 I'm not so sure that I believe in the benefit of overweighting.
  In standard dependency graphs, for example, modifications are
  usually dependendents of the head and are thus ``underweighted'' by
  your scheme, but modifiers like negations etc. can considerably
  change the meaning.  (You write ``we argue'' but I don't see an
  argument, just a claim -- details of counts for an example might be
  helpful.)
  \begin{response}
  We have added a sentence at the end of the paragraph pointing to the example
to motivate the ``overweighting''. (now 1st paragraph on page 4)
  \end{response}

\item p.5 ``When multiple decompositions etc.'' As far as I can see, you
  get consistently better scores for F[] than for $\mu_{PR}[]$
  throughout your experiments.  I'd suggest reducing the discussion of
  $\mu$ here and in Section 3.2 with a short explanation that $\mu$
  has been found to work worse than F.
  \begin{response}
  We have compressed the discussion somewhat in section 2, substantially in section 3 (to a single line), and combined the original Tables I and II without the $\mu$ results, and then mention $\mu$ findings in a sentence of the text. Since
this finding is significant and provides insight into differences between other methods in terms of score combination, we did not want to eliminate it entirely.
  \end{response}

\item  p.6 ``LFG...heavily knowledge-engineered parser''. I think you're
  being overly harsh on the LFG-based work here. As the title of the
  paper you cite says, you're talking about an ``automatically acquired
  PCFG-based LFG approximation'' that's learned from a treebank, so you
  are really very much in the same boat.
  \begin{response}
    Both are indeed built off treebanks, so the knowledge-engineering
    is similar. The contrast is in fact between a more ``surface''
    syntactic representation (the PCFG model) or a ``deeper'', more
    semantic one, and the discussion has been changed to reflect this.
  \end{response}

\item p.7 ``Score combination'' --- see comment above 
  \begin{response}
    As mentioned above, this was reduced to one line.
  \end{response}

\item ``Tuned, like the ...''  --- ungrammatical sentence?
  \begin{response}
    Clarified.
  \end{response}

\item  Evaluation (p. 8) In principle I like the idea of teasing apart
  sentence difficulty and translation quality. However, I have two
  reservations: (1), you are losing comparability of scores across
  sentences; and (2), you are committing to a scenario where you
  always have access to a sufficient number of translations in order
  to estimate the mean score. In other words, in the limit of one
  system, you do not predict anything.  These issues should be
  mentioned and discussed.  
  \begin{response}
    This method is used for evaluating the evaluation metric, not for
    evaluation itself, so comparability of scores across sentences is
    not really an issue for us.  That said, we agree that mean 
    normalization is not possible in all
    scenarios and in fact did not use it in section four.  Hence, we
    moved the mean normalization description to section 5, since it
    is only used when we have multiple translations.  
  \end{response}
  (Note about presentation: you don't
  introduce the variable $t$ in Equation 3 -- I guess this is the
  segment?)
  \begin{response}
    Fixed, but to save space the equation is now inline and not numbered.
  \end{response}
 
  More generally, I would like to see a motivation for the evaluation
  you perform. You seem to largely follow Owczarzak et al's
  evaluation, e.g. in the choice of data (Section 4.1), but you depart
  from it in some crucial respects (e.g. the definition of the
  dependent variable by mean subtraction). Can you provide a clearer
  description of why each experiment is set up in the way it is?
  \begin{response}
    Section 4 is intended to match the Owczarzak \textit{et al.} paradigm
    as closely as possible, but sections 5 and 6 do not since the focus 
    is on HTER. As mentioned above, mean subtraction is {\bf not} used 
    in section 4 and discussion of that was moved for clarity.  We think
    that this concern is really a lack of clarity in the original submission
    which has hopefully now been fixed.
  \end{response}

\item  p.10 Am I right in reading Table II to say that F always
  outperforms BLEU, but $\mu$ never does? Maybe it would be good to
  re-mention the BLEU results in Table II.
  \begin{response}
   Yes.  This is no longer in the tables because of our response to
   your earlier comment abut F vs. $\mu$, but we include this observation
   in the text (end of second paragraph, section 4)
  \end{response}

\item  p.12 Can you elaborate on why you want to weight per-sentence
  correlations by sentence length? I don't feel that this
  appropriate. After all, I'd think that you are not \emph{more}
  interested in longer sentences, but you want each sentence to have
  the same contribution to the analysis. I assume the problem is that
  there is considerably more variance for shorter sentence, which is
  alleviated by length-based weighting, but to me this reads like a
  cover-up. If you have good arguments for wanting long sentences to
  count more in the analysis, I'd be interested to hear them.
  \begin{response}
  We have tried to make it more clear that only the sentence-level results (not document level) use length-normalized scores in computing correlations. We explain that this is ``in order to get a per-word measure which reduces variance across sentences'' and we clarify that this is not giving the longer sentences more weight but rather the same weight by using the term ``normalized'' rather than ``weighted'' and saying that it is per-word.
  \end{response}

\item  Section 6. To my mind, this is the least convincing section of the
  paper. The experiments in Section 4 have already shown that a simple
  F-score-based combination of the different matches does better than
  considering individual components. While I agree that this might be
  a weighting problem, this experiment shows basically a null result.
  From what I can gather from Figure 4, TERp on its own does
  (significantly?) better than EDPM, and their combination doesn't
  really improve a lot. I acknowledge the positive result that there
  is no \emph{decrease} in performance either, but then you're using
  quite a lot of data (50\% = 2500 sentences) for tuning so that is to
  be expected. I would very much like to see how robust the tuning
  process is to smaller development sets, but that would require even
  more space.
\end{itemize}
In sum, I'd suggest replacing Section 6 with some actual examples and
more discussion of the methodical points I have suggested.
\begin{response}
  As described earlier, we have made a number of changes to section 6.  We have
  revisited the two-fold validation and provided a new statistical
  analysis on a smaller set of results that simplifies both the presentation (figure 4) and the
  discussion (elsewhere in section 6). In particular, the changes
  include:
  \begin{enumerate}
  \item dropping the discussion of $B$ brevity penalties, since these are not in the  referenced TERp and are not relevant to the point of the combining
    syntax and paraphrasing features
  \item collapsing the $T$ (edit and synonymy) and $P$ (paraphrasing)
    categories together (now presented together as $T$), again since the
    separation does not contribute to the point of the paper
  \item folding together and normalizing the values produced for the
    cross-validation-tuned combination measures to provide a single value and
    confidence interval rather than presenting both splits, which leads to a simpler 
    figure with more significant differences.
  \item adding a motivational introductory paragraph
  \end{enumerate}
\end{response}
\end{document}
