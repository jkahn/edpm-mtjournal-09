% -*- LaTeX -*-
\documentclass[letterpaper,12pt]{article}

\newenvironment{response}
{\begin{quotation} \it}
  {\end{quotation}}

\begin{document}

\section{Comments for the Author: Reviewer \#1}
\label{sec:reviewer1}

This submission on Dependency Pair Match MT metrics is technically and
structurally sound, thorough, and well-written.  Some specific
comments/questions below, in order of appearance in the paper.
\begin{response}
  Thank you for this feedback. Our responses are below.
\end{response}
As per the submission guidelines, submissions must be limited to 12
pages of content, with an additional page for references.  The current
version has 16.5 pages plus one page of references, so significant
shortening will be necessary.  I find the overall balance of sections
adequate, so I would recommend shortening across the board rather than
in one particular section.
\begin{response}
  We have done revisions to compress and simplify the writing
  throughout the paper to address the length concerns.
\end{response}
While likely known by most of the community, I would recommend
expanding all acronyms the first time (LFG, PCFG, ...).

In section 2, ``bag of dependent word'' should be ``bag of dependent
word*s*''.
\begin{response}
  We believe that we have emended these.
\end{response}
Can you expand on the reasoning behind the modifications to the
Charniak parser rules described in section 3.1?
\begin{response}
  We hope that our examples (and some additional explanatory sentences
  in section 3) have addressed this concern.
\end{response}
In section 4.1, while the corpus is referenced and the information can
be looked up, it would still be helpful to include directly in the
paper a brief description of the adequacy and fluency measures.  In
particular, what kind (granularity) of scale was used for the
judgments needs to be known in order to be able to assess the reported
correlations.  A brief description of the data genre(s) would be
useful to have at hand as well.  Also, I assume you correlate at the
*segment* level, but this is not explicitly stated.
\begin{response}
  Section 4 has now included several additional sentences to clarify
  these points.
\end{response}
The correlations reported in section 4 (tables I-III) are overall
quite low (also compared to EDPM's r correlations for Chinese source
data in NIST's MetricsMATR challenge).  Could you comment on this?
\begin{response}
 [TO DO]
\end{response}
For ease of lookup, please include in the table headings for tables
I-III that the correlations are with human judgments of adequacy and
fluency

In section 5.1, ``EDPM's strengths, relative to the other measures, are
particularly clear in the unstructured domains (wb and bc).''  This
statement does not seem to hold for EDPM vs. BLEU for the wb genre in
table Va; the correlation reported is the same for the two metrics?

\section{Comments for the Author: Reviewer \#2}
\label{sec:reviewer2}

This article presents a method for evaluating MT system hypotheses
that is based on complete and partial dependency triple matches
between hypothesis and reference translation.

The paper is well motivated, clearly written, and presents interesting
results. On my opinion, it is suitable for publication with minor
revisions. My main points of criticism are:

\begin{itemize}
\item It is considerably too long. Papers are supposed to be 12 pages
  + references; this paper has more than 16 pages. I recommend (a)
  shortening the introduction; (b) shortening the discussion of the
  ``model space'' in Sections 2/3 somewhat; (c) removing the third
  experiment (Section 6), whose contribution is unclear to me,
  altogether. (see below for details)

\item On the other hand, the paper does not motivate at all what the
  intuition behind the use of dependency triple match is. Why should
  we assume in the first place that dependency triples predict human
  judgments better than (e.g.) n-grams?

\item Finally, the paper does not give a single example. In a paper
  about a novel evaluation metric, I would like to get a feeling for
  what it is that the new metric does better than existing
  metrics. (This can be addressed together with the previous point.)
\end{itemize}

Detail comments:

\begin{itemize}
\item The discussion on page 2 reads nicely, but is probably too
  extensive -- in particular for a paper aimed at a special issue
  where there will presumably be an overview article that discusses
  relevant previous work.

\item  p.2, publically $\to$ publicly?
  \begin{response}
    This spelling error has been corrected.
  \end{response}

\item  p.3, ``In section 2 etc.'' I'm not sure that a 12-page paper
  requires a table of contents.

\item  p.3, reference to Owczarzak et al.'s work. In particular since
  their work has also appeared in the MT journal (please update the
  reference, btw!), it would be good to expand on the similarities and
  differences of your work and theirs.

\item p.4 I'm not so sure that I believe in the benefit of overweighting.
  In standard dependency graphs, for example, modifications are
  usually dependendents of the head and are thus ``underweighted'' by
  your scheme, but modifiers like negations etc. can considerably
  change the meaning.  (You write ``we argue'' but I don't see an
  argument, just a claim -- details of counts for an example might be
  helpful.)

\item p.5 ``When multiple decompositions etc.'' As far as I can see, you
  get consistently better scores for F[] than for $\mu_{PR}[]$
  throughout your experiments.  I'd suggest reducing the discussion of
  $\mu$ here and in Section 3.2 with a short explanation that $\mu$
  has been found to work worse than F.

\item  p.6 ``LFG...heavily knowledge-engineered parser''. I think you're
  being overly harsh on the LFG-based work here. As the title of the
  paper you cite says, you're talking about an ``automatically acquired
  PCFG-based LFG approximation'' that's learned from a treebank, so you
  are really very much in the same boat.

\item p.7 ``Score combination'' --- see comment above 

\item ``Tuned, like the ...''  --- ungrammatical sentence?

\item  Evaluation (p. 8) In principle I like the idea of teasing apart
  sentence difficulty and translation quality. However, I have two
  reservations: (1), you are losing comparability of scores across
  sentences; and (2), you are committing to a scenario where you
  always have access to a sufficient number of translations in order
  to estimate the mean score. In other words, in the limit of one
  system, you do not predict anything.  These issues should be
  mentioned and discussed.  (Note about presentation: you don't
  introduce the variable t in Equation 3 -- I guess this is the
  segment?)
 
  More generally, I would like to see a motivation for the evaluation
  you perform. You seem to largely follow Owczarzak et al's
  evaluation, e.g. in the choice of data (Section 4.1), but you depart
  from it in some crucial respects (e.g. the definition of the
  dependent variable by mean subtraction). Can you provide a clearer
  description of why each experiment is set up in the way it is?

\item  p.10 Am I right in reading Table II to say that F always
  outperforms BLEU, but $\mu$ never does? Maybe it would be good to
  re-mention the BLEU results in Table II.

\item  p.12 Can you elaborate on why you want to weight per-sentence
  correlations by sentence length? I don't feel that this
  appropriate. After all, I'd think that you are not \emph{more}
  interested in longer sentences, but you want each sentence to have
  the same contribution to the analysis. I assume the problem is that
  there is considerably more variance for shorter sentence, which is
  alleviated by length-based weighting, but to me this reads like a
  cover-up. If you have good arguments for wanting long sentences to
  count more in the analysis, I'd be interested to hear them.

\item  Section 6. To my mind, this is the least convincing section of the
  paper. The experiments in Section 4 have already shown that a simple
  F-score-based combination of the different matches does better than
  considering individual components. While I agree that this might be
  a weighting problem, this experiment shows basically a null result.
  From what I can gather from Figure 4, TERp on its own does
  (significantly?) better than EDPM, and their combination doesn't
  really improve a lot. I acknowledge the positive result that there
  is no \emph{decrease} in performance either, but then you're using
  quite a lot of data (50\% = 2500 sentences) for tuning so that is to
  be expected. I would very much like to see how robust the tuning
  process is to smaller development sets, but that would require even
  more space.
\end{itemize}
 In sum, I'd suggest replacing Section 6 with some actual examples
 and more discussion of the methodical points I have suggested.
\end{document}